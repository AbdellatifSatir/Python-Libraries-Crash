{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b690349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b58928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ae1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "sent_tokenize = sent_tokenize(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi\")\n",
    "word_tokenize = word_tokenize(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e18829",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11379020",
   "metadata": {},
   "source": [
    "### spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e82c4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x26d7723dc70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d6c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "chaat\n",
      "of\n",
      "delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827f1a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "dir(doc[0]) # all methods available\n",
    "print(type(doc))\n",
    "print(type(doc[1:4]))\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c14c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp('i bought two laptops for my job on 2000$')\n",
    "doc2[2].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba0cd9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[-1].is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887f8ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chaat of delhi.\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')\n",
    "nlp.pipe_names\n",
    "doc = nlp('Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi.')\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63dbb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2aaa2b",
   "metadata": {},
   "source": [
    "### spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4edd278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x26d776b1a00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "nlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "744d6ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5afddaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = nlp('Dr. Strange loves pav bhaji of mumbai. Hulk loves chaat of delhi')\n",
    "new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "674ebf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chaat of delhi.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d62ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "chaat\n",
      "of\n",
      "delhi\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ee29",
   "metadata": {},
   "source": [
    "#### POS - Part Of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f46859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain \t PROPN \t | proper noun \t | NNP \t | noun, proper singular\n",
      "america \t PROPN \t | proper noun \t | NNP \t | noun, proper singular\n",
      "ate \t VERB \t | verb \t | VBD \t | verb, past tense\n",
      "100 \t NUM \t | numeral \t | CD \t | cardinal number\n",
      "$ \t SYM \t | symbol \t | $ \t | symbol, currency\n",
      "samosa \t NOUN \t | noun \t | NNS \t | noun, plural\n",
      ". \t PUNCT \t | punctuation \t | . \t | punctuation mark, sentence closer\n",
      "Then \t ADV \t | adverb \t | RB \t | adverb\n",
      "he \t PRON \t | pronoun \t | PRP \t | pronoun, personal\n",
      "said \t VERB \t | verb \t | VBD \t | verb, past tense\n",
      "i \t PRON \t | pronoun \t | PRP \t | pronoun, personal\n",
      "can \t AUX \t | auxiliary \t | MD \t | verb, modal auxiliary\n",
      "do \t VERB \t | verb \t | VB \t | verb, base form\n",
      "this \t PRON \t | pronoun \t | DT \t | determiner\n",
      "all \t DET \t | determiner \t | DT \t | determiner\n",
      "day \t NOUN \t | noun \t | NN \t | noun, singular or mass\n",
      ". \t PUNCT \t | punctuation \t | . \t | punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "txt = nlp2(\"Captain america ate 100$ samosa. Then he said i can do this all day.\")\n",
    "\n",
    "for token in txt:\n",
    "    print(token , '\\t' , token.pos_ , '\\t |' , spacy.explain(token.pos_) , '\\t |' , token.tag_ , \n",
    "          '\\t |' , spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cfdfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = txt.count_by(spacy.attrs.POS)\n",
    "\n",
    "# for i,j in count.items():\n",
    "#     print(txt.vacab[i].text,'\\t|',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c35f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ee1d26",
   "metadata": {},
   "source": [
    "#### NER - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c96ec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc \t ORG \t Companies, agencies, institutions, etc.\n",
      "Tesla Inc \t ORG \t Companies, agencies, institutions, etc.\n",
      "Twitter \t PRODUCT \t Objects, vehicles, foods, etc. (not services)\n",
      "45$ billion \t MONEY \t Monetary values, including unit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    45$ billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt2 = nlp2(\"Apple Inc and Tesla Inc is going to acquire Twitter for 45$ billion\")\n",
    "\n",
    "for ent in txt2.ents:\n",
    "        print(ent.text ,'\\t', ent.label_ ,'\\t', spacy.explain(ent.label_))\n",
    "        \n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(txt2,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f72fb4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple Inc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(txt2,0,2)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1cc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62496a4e",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization\n",
    "- Stem : talking -> talk\n",
    "- Lemma : ate -> eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c4de63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats \t eat\n",
      "eating \t eat\n",
      "eat \t eat\n",
      "ate \t ate\n",
      "adjustable \t adjust\n",
      "rafting \t raft\n",
      "ability \t abil\n",
      "meeting \t meet\n"
     ]
    }
   ],
   "source": [
    "#spacy doent have support for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"eats\",\"eating\",\"eat\",\"ate\",\"adjustable\",\"rafting\",\"ability\",\"meeting\"]\n",
    "for word in words:\n",
    "    print(word,'\\t',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "230bf81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats \t eats\n",
      "eating \t eat\n",
      "eat \t eat\n",
      "ate \t eat\n",
      "adjustable \t adjustable\n",
      "rafting \t raft\n",
      "ability \t ability\n",
      "meeting \t meeting\n",
      "better \t well\n",
      "is \t be\n",
      "was \t be\n",
      "bro \t bro\n",
      "bruh \t bruh\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "lemmas = nlp2(\"eats eating eat ate adjustable rafting ability meeting better is was bro bruh\")\n",
    "\n",
    "for word in lemmas:\n",
    "    print(word, '\\t', word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db63d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats \t eats\n",
      "eating \t eat\n",
      "eat \t eat\n",
      "ate \t eat\n",
      "adjustable \t adjustable\n",
      "rafting \t raft\n",
      "ability \t ability\n",
      "meeting \t meeting\n",
      "better \t well\n",
      "is \t be\n",
      "was \t be\n",
      "bro \t Brother\n",
      "bruh \t Brother\n"
     ]
    }
   ],
   "source": [
    "nlp2.pipe_names\n",
    "attribute_ruler = nlp2.get_pipe('attribute_ruler')\n",
    "attribute_ruler.add([[{\"TEXT\" : \"bro\"}] , [{\"TEXT\" : \"bruh\"}]],\n",
    "                    {\"LEMMA\" : \"Brother\"})\n",
    "\n",
    "lemmas = nlp2(\"eats eating eat ate adjustable rafting ability meeting better is was bro bruh\")\n",
    "\n",
    "for word in lemmas:\n",
    "    print(word, '\\t', word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d39db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb3199b",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb348682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened\n",
      "wings\n",
      "flying\n",
      "coming\n",
      "soon\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "len(STOP_WORDS)\n",
    "\n",
    "txt = \"we just opened our wings, the flying part is coming soon\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(txt)\n",
    "\n",
    "type(doc)\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9492040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78de5294",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "200b688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "and\n",
      "document\n",
      "first\n",
      "is\n",
      "one\n",
      "second\n",
      "the\n",
      "third\n",
      "this\n",
      "\n",
      "Vector representation of the documents:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create an instance of the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn the vocabulary and transform the documents into vectors\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "for feature in feature_names:\n",
    "    print(feature)\n",
    "\n",
    "# Print the vector representation of the documents\n",
    "print(\"\\nVector representation of the documents:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378ddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403b2f03",
   "metadata": {},
   "source": [
    "## Bag Of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5191fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "enjoy\n",
      "enjoy learning\n",
      "fascinating\n",
      "is\n",
      "is fascinating\n",
      "learning\n",
      "learning nlp\n",
      "love\n",
      "love nlp\n",
      "nlp\n",
      "nlp is\n",
      "\n",
      "Bag of n-grams representation of the documents:\n",
      "[[0 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 1 1 1 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fascinating\",\n",
    "    \"I enjoy learning NLP\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer with n-gram range (e.g., unigrams and bigrams)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Learn the vocabulary and transform the documents into a Bag of n-grams representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (n-grams in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "for feature in feature_names:\n",
    "    print(feature)\n",
    "\n",
    "# Print the Bag of n-grams representation of the documents\n",
    "print(\"\\nBag of n-grams representation of the documents:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f505dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47aa4831",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c961fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "and\n",
      "document\n",
      "first\n",
      "is\n",
      "one\n",
      "second\n",
      "the\n",
      "third\n",
      "this\n",
      "\n",
      "TF-IDF representation of the documents:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn the vocabulary and transform the documents into a TF-IDF representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (terms in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "for feature in feature_names:\n",
    "    print(feature)\n",
    "\n",
    "# Print the TF-IDF representation of the documents\n",
    "print(\"\\nTF-IDF representation of the documents:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f53a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb2858e5",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "568407df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example corpus\n",
    "corpus = [[\"I\", \"love\", \"NLP\"],\n",
    "          [\"Word\", \"embeddings\", \"are\", \"useful\"],\n",
    "          [\"Machine\", \"learning\", \"is\", \"interesting\"],\n",
    "          [\"king\", \"man\", \"woman\", \"queen\"]]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv[\"NLP\"]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"learning\")\n",
    "\n",
    "# Perform vector arithmetic\n",
    "result = model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30bafb96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.14595064520835876),\n",
       " ('embeddings', 0.05048208683729172),\n",
       " ('king', 0.04157736897468567),\n",
       " ('interesting', 0.03476494178175926),\n",
       " ('is', 0.019152291119098663),\n",
       " ('queen', 0.01613471284508705),\n",
       " ('love', 0.01281161978840828),\n",
       " ('useful', 0.00882619246840477),\n",
       " ('I', 0.007729304954409599),\n",
       " ('Machine', 0.004842511378228664)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f3477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47591792",
   "metadata": {},
   "source": [
    "## Word vector in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d0f27a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog \tVector True \tOOV False\n",
      "cat \tVector True \tOOV False\n",
      "banana \tVector True \tOOV False\n",
      "hejfhz \tVector False \tOOV True\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp('dog cat banana hejfhz')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,'\\tVector',token.has_vector,'\\tOOV',token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "effcb770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread <-> bread: 1.0\n",
      "sandwich <-> bread: 0.6341067010130894\n",
      "burger <-> bread: 0.47520687769584247\n",
      "car <-> bread: 0.06451532596945217\n",
      "tiger <-> bread: 0.04764611272488976\n",
      "human <-> bread: 0.2151154210812192\n",
      "wheat <-> bread: 0.615036141030184\n"
     ]
    }
   ],
   "source": [
    "base_token = nlp('bread')\n",
    "\n",
    "doc2 = nlp('bread sandwich burger car tiger human wheat')\n",
    "\n",
    "for token in doc2:\n",
    "    print(f'{token.text} <-> {base_token.text}:', token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a982f176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6178014]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "result = king - man + woman\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([result] , [queen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270f09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7225d483",
   "metadata": {},
   "source": [
    "## Word vector in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea8d5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "wv.similarity(w1=\"great\",w2=\"good\")\n",
    "wv.most_similar('good')\n",
    "# king - women + man = queen\n",
    "# franse - paris + berlin = germany\n",
    "wv.most_similar(positive=['Franse','Berlin'],negative='Paris')\n",
    "wv.doesnt_match(['facebook','cat','google','microsoft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "494a7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv = api.load('glove-twitter-25')\n",
    "glv.most_similar('good')\n",
    "glv.doesnt_match('facebook cat google microsoft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418a507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0d7b62",
   "metadata": {},
   "source": [
    "## Word vector in fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346add5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b65ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51528004",
   "metadata": {},
   "source": [
    "## Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc1771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e243af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
